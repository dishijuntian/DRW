{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install koolbox scikit-learn==1.5.2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import clone\n",
    "from koolbox import Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class CFG:\n",
    "    train_path = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n",
    "    test_path = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n",
    "    sample_sub_path = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n",
    "\n",
    "    target = \"label\"\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "\n",
    "    run_optuna = True\n",
    "    n_optuna_trials = 250\n",
    "\n",
    "def reduce_mem_usage(dataframe, dataset):    \n",
    "    print('Reducing memory usage for:', dataset)\n",
    "    initial_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "\n",
    "        c_min = dataframe[col].min()\n",
    "        c_max = dataframe[col].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.float32)\n",
    "            else:\n",
    "                dataframe[col] = dataframe[col].astype(np.float64)\n",
    "\n",
    "    final_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "    print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
    "    print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
    "    print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "cols_to_drop = [\n",
    "    'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', \n",
    "    'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716',\n",
    "    'X717', 'X864', 'X867', 'X869', 'X870', 'X871', 'X872', 'X104', 'X110', 'X116',\n",
    "    'X122', 'X128', 'X134', 'X140', 'X146', 'X152', 'X158', 'X164', 'X170', 'X176',\n",
    "    'X182', 'X351', 'X357', 'X363', 'X369', 'X375', 'X381', 'X387', 'X393', 'X399',\n",
    "    'X405', 'X411', 'X417', 'X423', 'X429'\n",
    "]\n",
    "\n",
    "train = pd.read_parquet(CFG.train_path).reset_index(drop=True)\n",
    "test = pd.read_parquet(CFG.test_path).reset_index(drop=True)\n",
    "\n",
    "train = train.drop(columns=cols_to_drop)\n",
    "test = test.drop(columns=[\"label\"] + cols_to_drop)\n",
    "\n",
    "train = reduce_mem_usage(train, \"train\")\n",
    "test = reduce_mem_usage(test, \"test\")\n",
    "\n",
    "# Create interaction features for train\n",
    "train['bid_ask_interaction'] = train['bid_qty'] * train['ask_qty']\n",
    "train['bid_buy_interaction'] = train['bid_qty'] * train['buy_qty']\n",
    "train['bid_sell_interaction'] = train['bid_qty'] * train['sell_qty']\n",
    "train['ask_buy_interaction'] = train['ask_qty'] * train['buy_qty']\n",
    "train['ask_sell_interaction'] = train['ask_qty'] * train['sell_qty']\n",
    "train['buy_sell_interaction'] = train['buy_qty'] * train['sell_qty']\n",
    "\n",
    "# Calculate spread indicators for train\n",
    "train['spread_indicator'] = (train['ask_qty'] - train['bid_qty']) / (train['ask_qty'] + train['bid_qty'] + 1e-8)\n",
    "\n",
    "# Volume-weighted features for train\n",
    "train['volume_weighted_buy'] = train['buy_qty'] * train['volume']\n",
    "train['volume_weighted_sell'] = train['sell_qty'] * train['volume']\n",
    "train['volume_weighted_bid'] = train['bid_qty'] * train['volume']\n",
    "train['volume_weighted_ask'] = train['ask_qty'] * train['volume']\n",
    "\n",
    "# NEW FEATURES - Add ratio features\n",
    "train['buy_sell_ratio'] = train['buy_qty'] / (train['sell_qty'] + 1e-8)\n",
    "train['bid_ask_ratio'] = train['bid_qty'] / (train['ask_qty'] + 1e-8)\n",
    "\n",
    "# NEW FEATURES - Add order flow imbalance\n",
    "train['order_flow_imbalance'] = (train['buy_qty'] - train['sell_qty']) / (train['volume'] + 1e-8)\n",
    "\n",
    "# NEW FEATURES - Add market pressure indicators\n",
    "train['buying_pressure'] = train['buy_qty'] / (train['volume'] + 1e-8)\n",
    "train['selling_pressure'] = train['sell_qty'] / (train['volume'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Liquidity measures\n",
    "train['total_liquidity'] = train['bid_qty'] + train['ask_qty']\n",
    "train['liquidity_imbalance'] = (train['bid_qty'] - train['ask_qty']) / (train['total_liquidity'] + 1e-8)\n",
    "train['relative_spread'] = (train['ask_qty'] - train['bid_qty']) / (train['volume'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Trade intensity\n",
    "train['trade_intensity'] = (train['buy_qty'] + train['sell_qty']) / (train['volume'] + 1e-8)\n",
    "train['avg_trade_size'] = train['volume'] / (train['buy_qty'] + train['sell_qty'] + 1e-8)\n",
    "train['net_trade_flow'] = (train['buy_qty'] - train['sell_qty']) / (train['buy_qty'] + train['sell_qty'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Market depth and activity\n",
    "train['depth_ratio'] = train['total_liquidity'] / (train['volume'] + 1e-8)\n",
    "train['volume_participation'] = (train['buy_qty'] + train['sell_qty']) / (train['total_liquidity'] + 1e-8)\n",
    "train['market_activity'] = train['volume'] * train['total_liquidity']\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Execution quality indicators\n",
    "train['effective_spread_proxy'] = np.abs(train['buy_qty'] - train['sell_qty']) / (train['volume'] + 1e-8)\n",
    "train['realized_volatility_proxy'] = np.abs(train['order_flow_imbalance']) * train['volume']\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Normalized volumes\n",
    "train['normalized_buy_volume'] = train['buy_qty'] / (train['bid_qty'] + 1e-8)\n",
    "train['normalized_sell_volume'] = train['sell_qty'] / (train['ask_qty'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES - Complex interactions\n",
    "train['liquidity_adjusted_imbalance'] = train['order_flow_imbalance'] * train['depth_ratio']\n",
    "train['pressure_spread_interaction'] = train['buying_pressure'] * train['spread_indicator']\n",
    "\n",
    "# Replace any inf or -inf values with NaN, then fill NaN with 0\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "train = train.fillna(0)\n",
    "\n",
    "# Create same features for test\n",
    "test['bid_ask_interaction'] = test['bid_qty'] * test['ask_qty']\n",
    "test['bid_buy_interaction'] = test['bid_qty'] * test['buy_qty']\n",
    "test['bid_sell_interaction'] = test['bid_qty'] * test['sell_qty']\n",
    "test['ask_buy_interaction'] = test['ask_qty'] * test['buy_qty']\n",
    "test['ask_sell_interaction'] = test['ask_qty'] * test['sell_qty']\n",
    "test['buy_sell_interaction'] = test['buy_qty'] * test['sell_qty']\n",
    "\n",
    "# Calculate spread indicators for test\n",
    "test['spread_indicator'] = (test['ask_qty'] - test['bid_qty']) / (test['ask_qty'] + test['bid_qty'] + 1e-8)\n",
    "\n",
    "# Volume-weighted features for test\n",
    "test['volume_weighted_buy'] = test['buy_qty'] * test['volume']\n",
    "test['volume_weighted_sell'] = test['sell_qty'] * test['volume']\n",
    "test['volume_weighted_bid'] = test['bid_qty'] * test['volume']\n",
    "test['volume_weighted_ask'] = test['ask_qty'] * test['volume']\n",
    "\n",
    "# NEW FEATURES FOR TEST - Add ratio features\n",
    "test['buy_sell_ratio'] = test['buy_qty'] / (test['sell_qty'] + 1e-8)\n",
    "test['bid_ask_ratio'] = test['bid_qty'] / (test['ask_qty'] + 1e-8)\n",
    "\n",
    "# NEW FEATURES FOR TEST - Add order flow imbalance\n",
    "test['order_flow_imbalance'] = (test['buy_qty'] - test['sell_qty']) / (test['volume'] + 1e-8)\n",
    "\n",
    "# NEW FEATURES FOR TEST - Add market pressure indicators\n",
    "test['buying_pressure'] = test['buy_qty'] / (test['volume'] + 1e-8)\n",
    "test['selling_pressure'] = test['sell_qty'] / (test['volume'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Liquidity measures\n",
    "test['total_liquidity'] = test['bid_qty'] + test['ask_qty']\n",
    "test['liquidity_imbalance'] = (test['bid_qty'] - test['ask_qty']) / (test['total_liquidity'] + 1e-8)\n",
    "test['relative_spread'] = (test['ask_qty'] - test['bid_qty']) / (test['volume'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Trade intensity\n",
    "test['trade_intensity'] = (test['buy_qty'] + test['sell_qty']) / (test['volume'] + 1e-8)\n",
    "test['avg_trade_size'] = test['volume'] / (test['buy_qty'] + test['sell_qty'] + 1e-8)\n",
    "test['net_trade_flow'] = (test['buy_qty'] - test['sell_qty']) / (test['buy_qty'] + test['sell_qty'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Market depth and activity\n",
    "test['depth_ratio'] = test['total_liquidity'] / (test['volume'] + 1e-8)\n",
    "test['volume_participation'] = (test['buy_qty'] + test['sell_qty']) / (test['total_liquidity'] + 1e-8)\n",
    "test['market_activity'] = test['volume'] * test['total_liquidity']\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Execution quality indicators\n",
    "test['effective_spread_proxy'] = np.abs(test['buy_qty'] - test['sell_qty']) / (test['volume'] + 1e-8)\n",
    "test['realized_volatility_proxy'] = np.abs(test['order_flow_imbalance']) * test['volume']\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Normalized volumes\n",
    "test['normalized_buy_volume'] = test['buy_qty'] / (test['bid_qty'] + 1e-8)\n",
    "test['normalized_sell_volume'] = test['sell_qty'] / (test['ask_qty'] + 1e-8)\n",
    "\n",
    "# ADDITIONAL NEW MARKET FEATURES FOR TEST - Complex interactions\n",
    "test['liquidity_adjusted_imbalance'] = test['order_flow_imbalance'] * test['depth_ratio']\n",
    "test['pressure_spread_interaction'] = test['buying_pressure'] * test['spread_indicator']\n",
    "\n",
    "# Replace any inf or -inf values with NaN, then fill NaN with 0\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.fillna(0)\n",
    "\n",
    "X = train.drop(CFG.target, axis=1)\n",
    "y = train[CFG.target]\n",
    "X_test = test\n",
    "\n",
    "# Ensure no inf values in X and X_test\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Force garbage collection before training\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "def _pearsonr(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "lgbm_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bytree\": 0.5625888953382505,\n",
    "    \"learning_rate\": 0.029312951475451557,\n",
    "    \"min_child_samples\": 63,\n",
    "    \"min_child_weight\": 0.11456572852335424,\n",
    "    \"n_estimators\": 126,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 37,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 85.2476527854083,\n",
    "    \"reg_lambda\": 99.38305361388907,\n",
    "    \"subsample\": 0.450669817684892,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "lgbm_goss_params = {\n",
    "    \"boosting_type\": \"goss\",\n",
    "    \"colsample_bytree\": 0.34695458228489784,\n",
    "    \"learning_rate\": 0.031023014900595287,\n",
    "    \"min_child_samples\": 30,\n",
    "    \"min_child_weight\": 0.4727729225033618,\n",
    "    \"n_estimators\": 220,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 58,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 38.665994901468224,\n",
    "    \"reg_lambda\": 92.76991677464294,\n",
    "    \"subsample\": 0.4810891284493255,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"colsample_bylevel\": 0.4778015829774066,\n",
    "    \"colsample_bynode\": 0.362764358742407,\n",
    "    \"colsample_bytree\": 0.7107423488010493,\n",
    "    \"gamma\": 1.7094857725240398,\n",
    "    \"learning_rate\": 0.02213323588455387,\n",
    "    \"max_depth\": 20,\n",
    "    \"max_leaves\": 12,\n",
    "    \"min_child_weight\": 16,\n",
    "    \"n_estimators\": 1667,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 39.352415706891264,\n",
    "    \"reg_lambda\": 75.44843704068275,\n",
    "    \"subsample\": 0.06566669853471274,\n",
    "    \"verbosity\": 0\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "oof_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train LightGBM (GBDT)\n",
    "print(\"\\n1. Training LightGBM (GBDT)...\")\n",
    "lgbm_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_params),\n",
    "    cv=KFold(n_splits=5, shuffle=False),\n",
    "    metric=_pearsonr,\n",
    "    task=\"regression\",\n",
    "    metric_precision=6\n",
    ")\n",
    "\n",
    "lgbm_trainer.fit(X, y)\n",
    "\n",
    "scores[\"LightGBM (gbdt)\"] = lgbm_trainer.fold_scores\n",
    "oof_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.oof_preds\n",
    "test_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.predict(X_test)\n",
    "\n",
    "print(f\"   Fold scores: {lgbm_trainer.fold_scores}\")\n",
    "print(f\"   Mean CV score: {np.mean(lgbm_trainer.fold_scores):.6f}\")\n",
    "print(f\"   Std CV score: {np.std(lgbm_trainer.fold_scores):.6f}\")\n",
    "\n",
    "# Free up memory\n",
    "del lgbm_trainer\n",
    "gc.collect()\n",
    "\n",
    "# Train LightGBM (GOSS)\n",
    "print(\"\\n2. Training LightGBM (GOSS)...\")\n",
    "lgbm_goss_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_goss_params),\n",
    "    cv=KFold(n_splits=5, shuffle=False),\n",
    "    metric=_pearsonr,\n",
    "    task=\"regression\",\n",
    "    metric_precision=6\n",
    ")\n",
    "\n",
    "lgbm_goss_trainer.fit(X, y)\n",
    "\n",
    "scores[\"LightGBM (goss)\"] = lgbm_goss_trainer.fold_scores\n",
    "oof_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.oof_preds\n",
    "test_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.predict(X_test)\n",
    "\n",
    "print(f\"   Fold scores: {lgbm_goss_trainer.fold_scores}\")\n",
    "print(f\"   Mean CV score: {np.mean(lgbm_goss_trainer.fold_scores):.6f}\")\n",
    "print(f\"   Std CV score: {np.std(lgbm_goss_trainer.fold_scores):.6f}\")\n",
    "\n",
    "# Free up memory\n",
    "del lgbm_goss_trainer\n",
    "gc.collect()\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\n3. Training XGBoost...\")\n",
    "xgb_trainer = Trainer(\n",
    "    XGBRegressor(**xgb_params),\n",
    "    cv=KFold(n_splits=5, shuffle=False),\n",
    "    metric=_pearsonr,\n",
    "    task=\"regression\",\n",
    "    metric_precision=6\n",
    ")\n",
    "\n",
    "xgb_trainer.fit(X, y)\n",
    "\n",
    "scores[\"XGBoost\"] = xgb_trainer.fold_scores\n",
    "oof_preds[\"XGBoost\"] = xgb_trainer.oof_preds\n",
    "test_preds[\"XGBoost\"] = xgb_trainer.predict(X_test)\n",
    "\n",
    "print(f\"   Fold scores: {xgb_trainer.fold_scores}\")\n",
    "print(f\"   Mean CV score: {np.mean(xgb_trainer.fold_scores):.6f}\")\n",
    "print(f\"   Std CV score: {np.std(xgb_trainer.fold_scores):.6f}\")\n",
    "\n",
    "# Free up memory\n",
    "del xgb_trainer\n",
    "gc.collect()\n",
    "\n",
    "def plot_weights(weights, title):\n",
    "    sorted_indices = np.argsort(weights[0])[::-1]\n",
    "    sorted_coeffs = np.array(weights[0])[sorted_indices]\n",
    "    sorted_model_names = np.array(list(oof_preds.keys()))[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, weights.shape[1] * 0.5))\n",
    "    ax = sns.barplot(x=sorted_coeffs, y=sorted_model_names, palette=\"RdYlGn_r\")\n",
    "\n",
    "    for i, (value, name) in enumerate(zip(sorted_coeffs, sorted_model_names)):\n",
    "        if value >= 0:\n",
    "            ax.text(value, i, f\"{value:.3f}\", va=\"center\", ha=\"left\", color=\"black\")\n",
    "        else:\n",
    "            ax.text(value, i, f\"{value:.3f}\", va=\"center\", ha=\"right\", color=\"black\")\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.set_xlim(xlim[0] - 0.1 * abs(xlim[0]), xlim[1] + 0.1 * abs(xlim[1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X = pd.DataFrame(oof_preds)\n",
    "X_test = pd.DataFrame(test_preds)\n",
    "joblib.dump(X, \"oof_preds.pkl\")\n",
    "joblib.dump(X_test, \"test_preds.pkl\")\n",
    "\n",
    "# Free up memory from original features\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "def objective(trial):    \n",
    "    params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1000),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-2),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        \"positive\": trial.suggest_categorical(\"positive\", [True, False])\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        Ridge(**params),\n",
    "        cv=KFold(n_splits=5, shuffle=False),\n",
    "        metric=_pearsonr,\n",
    "        task=\"regression\",\n",
    "        verbose=False\n",
    "    )\n",
    "    trainer.fit(X, y)\n",
    "    \n",
    "    return np.mean(trainer.fold_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ENSEMBLE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CFG.run_optuna:\n",
    "    print(\"\\nOptimizing Ridge hyperparameters with Optuna...\")\n",
    "    sampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=CFG.n_optuna_trials, n_jobs=-1, catch=(ValueError,))\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best CV score: {study.best_value:.6f}\")\n",
    "    \n",
    "    # Free up memory\n",
    "    del study\n",
    "    gc.collect()\n",
    "\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": best_params[\"alpha\"],\n",
    "        \"tol\": best_params[\"tol\"],\n",
    "        \"fit_intercept\": best_params[\"fit_intercept\"],\n",
    "        \"positive\": best_params[\"positive\"]\n",
    "    }\n",
    "else:\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n",
    "\n",
    "print(\"\\nTraining final Ridge ensemble...\")\n",
    "ridge_trainer = Trainer(\n",
    "    Ridge(**ridge_params),\n",
    "    cv=KFold(n_splits=5, shuffle=False),\n",
    "    metric=_pearsonr,\n",
    "    task=\"regression\",\n",
    "    metric_precision=6\n",
    ")\n",
    "\n",
    "ridge_trainer.fit(X, y)\n",
    "\n",
    "scores[\"Ridge (ensemble)\"] = ridge_trainer.fold_scores\n",
    "ridge_test_preds = ridge_trainer.predict(X_test)\n",
    "\n",
    "print(f\"   Fold scores: {ridge_trainer.fold_scores}\")\n",
    "print(f\"   Mean CV score: {np.mean(ridge_trainer.fold_scores):.6f}\")\n",
    "print(f\"   Std CV score: {np.std(ridge_trainer.fold_scores):.6f}\")\n",
    "\n",
    "ridge_coeffs = np.zeros((1, X.shape[1]))\n",
    "for m in ridge_trainer.estimators:\n",
    "    ridge_coeffs += m.coef_\n",
    "ridge_coeffs = ridge_coeffs / len(ridge_trainer.estimators)\n",
    "\n",
    "# Free up memory\n",
    "del ridge_trainer\n",
    "gc.collect()\n",
    "\n",
    "# Print summary of all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "mean_scores = scores_df.mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nModel Rankings (by mean CV score):\")\n",
    "print(\"-\" * 40)\n",
    "for i, (model, score) in enumerate(mean_scores.items(), 1):\n",
    "    std_score = scores_df[model].std()\n",
    "    print(f\"{i}. {model:20s} - Mean: {score:.6f} (Â±{std_score:.6f})\")\n",
    "\n",
    "print(\"\\nDetailed Fold Scores:\")\n",
    "print(\"-\" * 40)\n",
    "for model in mean_scores.index:\n",
    "    fold_scores = scores[model]\n",
    "    print(f\"\\n{model}:\")\n",
    "    for fold, score in enumerate(fold_scores, 1):\n",
    "        print(f\"   Fold {fold}: {score:.6f}\")\n",
    "\n",
    "# Plot the ensemble weights\n",
    "plot_weights(ridge_coeffs, \"Ridge Ensemble Coefficients\")\n",
    "\n",
    "# Save submission\n",
    "sub = pd.read_csv(CFG.sample_sub_path)\n",
    "sub[\"prediction\"] = ridge_test_preds\n",
    "submission_filename = f\"sub_ridge_{np.mean(scores['Ridge (ensemble)']):.6f}.csv\"\n",
    "sub.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Submission saved as: {submission_filename}\")\n",
    "print(f\"Final ensemble CV score: {np.mean(scores['Ridge (ensemble)']):.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualization of scores\n",
    "order = mean_scores.index.tolist()\n",
    "\n",
    "min_score = mean_scores.min()\n",
    "max_score = mean_scores.max()\n",
    "padding = (max_score - min_score) * 0.5\n",
    "lower_limit = min_score - padding\n",
    "upper_limit = max_score + padding\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, scores_df.shape[1] * 0.5))\n",
    "\n",
    "boxplot = sns.boxplot(data=scores_df, order=order, ax=axs[0], orient=\"h\", color=\"grey\")\n",
    "axs[0].set_title(f\"Fold Score Distribution\")\n",
    "axs[0].set_xlabel(\"\")\n",
    "axs[0].set_ylabel(\"\")\n",
    "\n",
    "barplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], color=\"grey\")\n",
    "axs[1].set_title(f\"Average Score\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[1].set_xlim(left=lower_limit, right=upper_limit)\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "for i, (score, model) in enumerate(zip(mean_scores.values, mean_scores.index)):\n",
    "    color = \"cyan\" if \"ensemble\" in model.lower() else \"grey\"\n",
    "    barplot.patches[i].set_facecolor(color)\n",
    "    boxplot.patches[i].set_facecolor(color)\n",
    "    barplot.text(score, i, round(score, 6), va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProcess completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
