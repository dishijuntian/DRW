{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953c364a",
   "metadata": {},
   "source": [
    "## ÂØºÂÖ•‰æùËµñÂ∫ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee70beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import pearsonr\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from koolbox import Trainer\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988fe3e",
   "metadata": {},
   "source": [
    "## ËøêË°åÈÖçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9392a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the prediction pipeline.\"\"\"\n",
    "    \n",
    "    # Updated data paths to match directory structure\n",
    "    train_path: str = \"../kaggle/input/X_train.parquet\"\n",
    "    train_y_path: str = \"../kaggle/input/y_train.parquet\"\n",
    "    test_path: str = \"../kaggle/input/X_test.parquet\"\n",
    "    sample_sub_path: str = \"../kaggle/input/y_test.parquet\"\n",
    "    \n",
    "    # Output paths\n",
    "    models_dir: str = \"../models/\"\n",
    "    results_dir: str = \"../result/\"\n",
    "    \n",
    "    # Model parameters\n",
    "    target: str = \"label\"\n",
    "    n_folds: int = 50\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Optimization parameters\n",
    "    run_optuna: bool = True\n",
    "    n_optuna_trials: int = 100  # Reduced for notebook execution\n",
    "    \n",
    "    # GPU settings\n",
    "    use_gpu: bool = True  # Set to False if GPU is not available\n",
    "    gpu_id: int = 0\n",
    "    \n",
    "    # Columns to drop\n",
    "    cols_to_drop: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        \n",
    "        if self.cols_to_drop is None:\n",
    "            self.cols_to_drop = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b680c5",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dcfeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles data loading, preprocessing, and feature engineering.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "    def reduce_memory_usage(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Optimize dataframe memory usage by downcasting numeric types.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            dataset_name: Name for logging purposes\n",
    "            \n",
    "        Returns:\n",
    "            Memory-optimized dataframe\n",
    "        \"\"\"\n",
    "        print(f'Reducing memory usage for: {dataset_name}')\n",
    "        initial_mem = df.memory_usage().sum() / 1024**2\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            c_min, c_max = df[col].min(), df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        \n",
    "        final_mem = df.memory_usage().sum() / 1024**2\n",
    "        reduction = 100 * (initial_mem - final_mem) / initial_mem\n",
    "        \n",
    "        print(f'--- Memory usage before: {initial_mem:.2f} MB')\n",
    "        print(f'--- Memory usage after: {final_mem:.2f} MB')\n",
    "        print(f'--- Decreased memory usage by {reduction:.1f}%\\n')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create interaction features between different quantity types.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic interactions\n",
    "        df['bid_ask_interaction'] = df['bid_qty'] * df['ask_qty']\n",
    "        df['bid_buy_interaction'] = df['bid_qty'] * df['buy_qty']\n",
    "        df['bid_sell_interaction'] = df['bid_qty'] * df['sell_qty']\n",
    "        df['ask_buy_interaction'] = df['ask_qty'] * df['buy_qty']\n",
    "        df['ask_sell_interaction'] = df['ask_qty'] * df['sell_qty']\n",
    "        df['buy_sell_interaction'] = df['buy_qty'] * df['sell_qty']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_market_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create advanced market microstructure features.\"\"\"\n",
    "        df = df.copy()\n",
    "        eps = 1e-8  # Small constant to avoid division by zero\n",
    "        \n",
    "        # Spread and ratio features\n",
    "        df['spread_indicator'] = (df['ask_qty'] - df['bid_qty']) / (df['ask_qty'] + df['bid_qty'] + eps)\n",
    "        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + eps)\n",
    "        df['bid_ask_ratio'] = df['bid_qty'] / (df['ask_qty'] + eps)\n",
    "        \n",
    "        # Volume-weighted features\n",
    "        for qty_type in ['buy', 'sell', 'bid', 'ask']:\n",
    "            df[f'volume_weighted_{qty_type}'] = df[f'{qty_type}_qty'] * df['volume']\n",
    "        \n",
    "        # Order flow and pressure indicators\n",
    "        df['order_flow_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['volume'] + eps)\n",
    "        df['buying_pressure'] = df['buy_qty'] / (df['volume'] + eps)\n",
    "        df['selling_pressure'] = df['sell_qty'] / (df['volume'] + eps)\n",
    "        \n",
    "        # Liquidity measures\n",
    "        df['total_liquidity'] = df['bid_qty'] + df['ask_qty']\n",
    "        df['liquidity_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['total_liquidity'] + eps)\n",
    "        df['relative_spread'] = (df['ask_qty'] - df['bid_qty']) / (df['volume'] + eps)\n",
    "        \n",
    "        # Trade intensity and execution quality\n",
    "        df['trade_intensity'] = (df['buy_qty'] + df['sell_qty']) / (df['volume'] + eps)\n",
    "        df['avg_trade_size'] = df['volume'] / (df['buy_qty'] + df['sell_qty'] + eps)\n",
    "        df['net_trade_flow'] = (df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + eps)\n",
    "        \n",
    "        # Market depth and activity\n",
    "        df['depth_ratio'] = df['total_liquidity'] / (df['volume'] + eps)\n",
    "        df['volume_participation'] = (df['buy_qty'] + df['sell_qty']) / (df['total_liquidity'] + eps)\n",
    "        df['market_activity'] = df['volume'] * df['total_liquidity']\n",
    "        \n",
    "        # Advanced indicators\n",
    "        df['effective_spread_proxy'] = np.abs(df['buy_qty'] - df['sell_qty']) / (df['volume'] + eps)\n",
    "        df['realized_volatility_proxy'] = np.abs(df['order_flow_imbalance']) * df['volume']\n",
    "        df['normalized_buy_volume'] = df['buy_qty'] / (df['bid_qty'] + eps)\n",
    "        df['normalized_sell_volume'] = df['sell_qty'] / (df['ask_qty'] + eps)\n",
    "        \n",
    "        # Complex interactions\n",
    "        df['liquidity_adjusted_imbalance'] = df['order_flow_imbalance'] * df['depth_ratio']\n",
    "        df['pressure_spread_interaction'] = df['buying_pressure'] * df['spread_indicator']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean data by handling infinite values and NaNs.\"\"\"\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(0)\n",
    "        return df\n",
    "    \n",
    "    def load_and_process_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load and process training and test data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (X_train, y_train, X_test)\n",
    "        \"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        X_train = pd.read_parquet(self.config.train_path).reset_index(drop=True)\n",
    "        X_test = pd.read_parquet(self.config.test_path).reset_index(drop=True)\n",
    "        y_train = pd.read_parquet(self.config.train_y_path).reset_index(drop=True)\n",
    "        \n",
    "        X_train = self.clean_data(X_train)\n",
    "        X_test = self.clean_data(X_test)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Test data shape: {X_test.shape}\")\n",
    "        print(f\"Training y shape: {y_train.shape}\")\n",
    "        print(f\"Number of features: {X_train.shape[1]}\")\n",
    "        \n",
    "        return X_train, y_train, X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27450ec2",
   "metadata": {},
   "source": [
    "# Âü∫Á°ÄÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d54e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\"Abstract base class for all models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.trainer = None\n",
    "        self.fold_scores = None\n",
    "        self.oof_preds = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_model_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return model parameters.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"Return model name.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _pearsonr(self, y_true, y_pred):\n",
    "        \"\"\"Pearson correlation coefficient.\"\"\"\n",
    "        return pearsonr(y_true, y_pred)[0]\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[float], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Train the model using cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features\n",
    "            y: Training target\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (fold_scores, oof_predictions)\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.get_model_name()}...\")\n",
    "        \n",
    "        model_params = self.get_model_params()\n",
    "        model_class = self._get_model_class()\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model_class(**model_params),\n",
    "            cv=KFold(n_splits=self.config.n_folds, shuffle=False),\n",
    "            metric=self._pearsonr,\n",
    "            task=\"regression\",\n",
    "            metric_precision=6\n",
    "        )\n",
    "        print(type(y))\n",
    "        print(y.columns)\n",
    "        self.trainer.fit(X, y['label'])\n",
    "        self.fold_scores = self.trainer.fold_scores\n",
    "        self.oof_preds = self.trainer.oof_preds\n",
    "        \n",
    "        print(f\"   Fold scores: {self.fold_scores}\")\n",
    "        print(f\"   Mean CV score: {np.mean(self.fold_scores):.6f}\")\n",
    "        print(f\"   Std CV score: {np.std(self.fold_scores):.6f}\")\n",
    "        \n",
    "        return self.fold_scores, self.oof_preds\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if self.trainer is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        return self.trainer.predict(X)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_model_class(self):\n",
    "        \"\"\"Return the model class.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LightGBMModel(BaseModel):\n",
    "    \"\"\"LightGBM model with GPU support.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, boosting_type: str = \"gbdt\"):\n",
    "        super().__init__(config)\n",
    "        self.boosting_type = boosting_type\n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        return f\"LightGBM ({self.boosting_type})\"\n",
    "    \n",
    "    def _get_model_class(self):\n",
    "        return LGBMRegressor\n",
    "    \n",
    "    def get_model_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return optimized LightGBM parameters with GPU support.\"\"\"\n",
    "        base_params = {\n",
    "            \"random_state\": self.config.seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": -1,\n",
    "            \"boosting_type\": self.boosting_type,\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if enabled\n",
    "        if self.config.use_gpu:\n",
    "            base_params[\"device\"] = \"gpu\"\n",
    "            base_params[\"gpu_platform_id\"] = 0\n",
    "            base_params[\"gpu_device_id\"] = self.config.gpu_id\n",
    "        \n",
    "        if self.boosting_type == \"gbdt\":\n",
    "            specific_params = {\n",
    "                \"colsample_bytree\": 0.5625888953382505,\n",
    "                \"learning_rate\": 0.029312951475451557,\n",
    "                \"min_child_samples\": 63,\n",
    "                \"min_child_weight\": 0.11456572852335424,\n",
    "                \"n_estimators\": 126,\n",
    "                \"num_leaves\": 37,\n",
    "                \"reg_alpha\": 85.2476527854083,\n",
    "                \"reg_lambda\": 99.38305361388907,\n",
    "                \"subsample\": 0.450669817684892,\n",
    "            }\n",
    "        else:  # goss\n",
    "            specific_params = {\n",
    "                \"colsample_bytree\": 0.34695458228489784,\n",
    "                \"learning_rate\": 0.031023014900595287,\n",
    "                \"min_child_samples\": 30,\n",
    "                \"min_child_weight\": 0.4727729225033618,\n",
    "                \"n_estimators\": 220,\n",
    "                \"num_leaves\": 58,\n",
    "                \"reg_alpha\": 38.665994901468224,\n",
    "                \"reg_lambda\": 92.76991677464294,\n",
    "                \"subsample\": 0.4810891284493255,\n",
    "            }\n",
    "        \n",
    "        return {**base_params, **specific_params}\n",
    "\n",
    "\n",
    "class XGBoostModel(BaseModel):\n",
    "    \"\"\"XGBoost model with GPU support.\"\"\"\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"XGBoost\"\n",
    "    \n",
    "    def _get_model_class(self):\n",
    "        return XGBRegressor\n",
    "    \n",
    "    def get_model_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return optimized XGBoost parameters with GPU support.\"\"\"\n",
    "        params = {\n",
    "            \"random_state\": self.config.seed,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbosity\": 0,\n",
    "            \"colsample_bylevel\": 0.4778015829774066,\n",
    "            \"colsample_bynode\": 0.362764358742407,\n",
    "            \"colsample_bytree\": 0.7107423488010493,\n",
    "            \"gamma\": 1.7094857725240398,\n",
    "            \"learning_rate\": 0.02213323588455387,\n",
    "            \"max_depth\": 20,\n",
    "            \"max_leaves\": 12,\n",
    "            \"min_child_weight\": 16,\n",
    "            \"n_estimators\": 1667,\n",
    "            \"reg_alpha\": 39.352415706891264,\n",
    "            \"reg_lambda\": 75.44843704068275,\n",
    "            \"subsample\": 0.06566669853471274,\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if enabled\n",
    "        if self.config.use_gpu:\n",
    "            params[\"tree_method\"] = \"gpu_hist\"\n",
    "            params[\"gpu_id\"] = self.config.gpu_id\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59081d66",
   "metadata": {},
   "source": [
    "## Êï¥ÂêàÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad3dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    \"\"\"Ridge regression ensemble model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.best_params = None\n",
    "        self.trainer = None\n",
    "        self.coefficients = None\n",
    "        \n",
    "    def _pearsonr(self, y_true, y_pred):\n",
    "        \"\"\"Pearson correlation coefficient.\"\"\"\n",
    "        return pearsonr(y_true, y_pred)[0]\n",
    "    \n",
    "    def _optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize Ridge hyperparameters using Optuna.\"\"\"\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"random_state\": self.config.seed,\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 0, 1000),\n",
    "                \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-2),\n",
    "                \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "                \"positive\": trial.suggest_categorical(\"positive\", [True, False])\n",
    "            }\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                Ridge(**params),\n",
    "                cv=KFold(n_splits=self.config.n_folds, shuffle=False),\n",
    "                metric=self._pearsonr,\n",
    "                task=\"regression\",\n",
    "                verbose=False\n",
    "            )\n",
    "            trainer.fit(X, y)\n",
    "            \n",
    "            return np.mean(trainer.fold_scores)\n",
    "        \n",
    "        print(\"Optimizing Ridge hyperparameters with Optuna...\")\n",
    "        sampler = optuna.samplers.TPESampler(seed=self.config.seed, multivariate=True)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(objective, n_trials=self.config.n_optuna_trials, n_jobs=-1, catch=(ValueError,))\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters found: {best_params}\")\n",
    "        print(f\"Best CV score: {study.best_value:.6f}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del study\n",
    "        gc.collect()\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def train(self, oof_predictions: Dict[str, np.ndarray], y: pd.Series) -> Tuple[List[float], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Train the ensemble model.\n",
    "        \n",
    "        Args:\n",
    "            oof_predictions: Dictionary of out-of-fold predictions from base models\n",
    "            y: Target values\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (fold_scores, coefficients)\n",
    "        \"\"\"\n",
    "        X_oof = pd.DataFrame(oof_predictions)\n",
    "\n",
    "        # Optimize hyperparameters if enabled\n",
    "        if self.config.run_optuna:\n",
    "            self.best_params = self._optimize_hyperparameters(X_oof, y)\n",
    "            ridge_params = {\n",
    "                \"random_state\": self.config.seed,\n",
    "                **self.best_params\n",
    "            }\n",
    "        else:\n",
    "            ridge_params = {\"random_state\": self.config.seed}\n",
    "        \n",
    "        print(\"Training final Ridge ensemble...\")\n",
    "        self.trainer = Trainer(\n",
    "            Ridge(**ridge_params),\n",
    "            cv=KFold(n_splits=self.config.n_folds, shuffle=False),\n",
    "            metric=self._pearsonr,\n",
    "            task=\"regression\",\n",
    "            metric_precision=6\n",
    "        )\n",
    "        \n",
    "        self.trainer.fit(X_oof, y)\n",
    "        \n",
    "        # Calculate average coefficients\n",
    "        self.coefficients = np.zeros((1, X_oof.shape[1]))\n",
    "        for estimator in self.trainer.estimators:\n",
    "            self.coefficients += estimator.coef_\n",
    "        self.coefficients = self.coefficients / len(self.trainer.estimators)\n",
    "        \n",
    "        fold_scores = self.trainer.fold_scores\n",
    "        print(f\"   Fold scores: {fold_scores}\")\n",
    "        print(f\"   Mean CV score: {np.mean(fold_scores):.6f}\")\n",
    "        print(f\"   Std CV score: {np.std(fold_scores):.6f}\")\n",
    "        \n",
    "        return fold_scores, self.coefficients\n",
    "    \n",
    "    def predict(self, test_predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Make ensemble predictions.\"\"\"\n",
    "        X_test = pd.DataFrame(test_predictions)\n",
    "        return self.trainer.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ad8ec",
   "metadata": {},
   "source": [
    "## ÂèØËßÜÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd40122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \"\"\"Handles all visualization tasks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_ensemble_weights(coefficients: np.ndarray, model_names: List[str], title: str = \"Ridge Ensemble Coefficients\"):\n",
    "        \"\"\"Plot ensemble model coefficients.\"\"\"\n",
    "        sorted_indices = np.argsort(coefficients[0])[::-1]\n",
    "        sorted_coeffs = coefficients[0][sorted_indices]\n",
    "        sorted_names = np.array(model_names)[sorted_indices]\n",
    "        \n",
    "        plt.figure(figsize=(10, len(model_names) * 0.5))\n",
    "        ax = sns.barplot(x=sorted_coeffs, y=sorted_names, palette=\"RdYlGn_r\")\n",
    "        \n",
    "        for i, (value, name) in enumerate(zip(sorted_coeffs, sorted_names)):\n",
    "            ha = \"left\" if value >= 0 else \"right\"\n",
    "            ax.text(value, i, f\"{value:.3f}\", va=\"center\", ha=ha, color=\"black\")\n",
    "        \n",
    "        xlim = ax.get_xlim()\n",
    "        ax.set_xlim(xlim[0] - 0.1 * abs(xlim[0]), xlim[1] + 0.1 * abs(xlim[1]))\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_model_performance(scores: Dict[str, List[float]]):\n",
    "        \"\"\"Plot model performance comparison.\"\"\"\n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        mean_scores = scores_df.mean().sort_values(ascending=False)\n",
    "        \n",
    "        # Calculate plot limits\n",
    "        min_score, max_score = mean_scores.min(), mean_scores.max()\n",
    "        padding = (max_score - min_score) * 0.5\n",
    "        lower_limit = min_score - padding\n",
    "        upper_limit = max_score + padding\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, len(scores) * 0.5))\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=scores_df, order=mean_scores.index, ax=ax1, orient=\"h\", color=\"grey\")\n",
    "        ax1.set_title(\"Fold Score Distribution\")\n",
    "        ax1.set_xlabel(\"\")\n",
    "        ax1.set_ylabel(\"\")\n",
    "        \n",
    "        # Bar plot\n",
    "        barplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=ax2, color=\"grey\")\n",
    "        ax2.set_title(\"Average Score\")\n",
    "        ax2.set_xlabel(\"\")\n",
    "        ax2.set_xlim(left=lower_limit, right=upper_limit)\n",
    "        ax2.set_ylabel(\"\")\n",
    "        \n",
    "        # Highlight ensemble model\n",
    "        for i, (score, model) in enumerate(zip(mean_scores.values, mean_scores.index)):\n",
    "            color = \"cyan\" if \"ensemble\" in model.lower() else \"grey\"\n",
    "            barplot.patches[i].set_facecolor(color)\n",
    "            ax1.patches[i].set_facecolor(color)\n",
    "            barplot.text(score, i, f\"{score:.6f}\", va=\"center\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_target_distribution(y_train: pd.Series):\n",
    "        \"\"\"Visualize target distribution.\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(y_train, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.title('Target Distribution')\n",
    "        plt.xlabel('Label Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(y_train, vert=True)\n",
    "        plt.title('Target Box Plot')\n",
    "        plt.ylabel('Label Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_model_correlations(oof_preds: pd.DataFrame):\n",
    "        \"\"\"Display correlation matrix of base model predictions.\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        correlation_matrix = oof_preds.corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, linewidths=0.5)\n",
    "        plt.title('Correlation Matrix of Base Model Predictions')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return correlation_matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_prediction_distributions(oof_preds: pd.DataFrame):\n",
    "        \"\"\"Analyze prediction distributions.\"\"\"\n",
    "        n_models = len(oof_preds.columns)\n",
    "        n_cols = 2\n",
    "        n_rows = (n_models + 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))\n",
    "        axes = axes.ravel() if n_models > 1 else [axes]\n",
    "        \n",
    "        for i, (model_name, predictions) in enumerate(oof_preds.items()):\n",
    "            if i < len(axes):\n",
    "                axes[i].hist(predictions, bins=30, alpha=0.7, color=f'C{i}', edgecolor='black')\n",
    "                axes[i].set_title(f'{model_name} Predictions Distribution')\n",
    "                axes[i].set_xlabel('Prediction Value')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_models, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151580c",
   "metadata": {},
   "source": [
    "## È¢ÑÊµã‰ªªÂä°ÁÆ°Á∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ef02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "class CryptoPredictor:\n",
    "    \"\"\"Main prediction pipeline class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.data_processor = DataProcessor(config)\n",
    "        self.visualizer = Visualizer()\n",
    "        \n",
    "        # Initialize models\n",
    "        self.models = [\n",
    "            LightGBMModel(config, boosting_type=\"gbdt\"),\n",
    "            LightGBMModel(config, boosting_type=\"goss\"),\n",
    "            XGBoostModel(config)\n",
    "        ]\n",
    "        \n",
    "        self.ensemble = EnsembleModel(config)\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_pipeline(self) -> str:\n",
    "        \"\"\"\n",
    "        Run the complete prediction pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Filename of the saved submission\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"CRYPTO MARKET PREDICTION PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load and process data\n",
    "        X_train, y_train, X_test = self.data_processor.load_and_process_data()\n",
    "        \n",
    "        # Train base models\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING BASE MODELS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        oof_predictions = {}\n",
    "        test_predictions = {}\n",
    "        all_scores = {}\n",
    "        \n",
    "        for model in self.models:\n",
    "            fold_scores, oof_preds = model.train(X_train, y_train)\n",
    "            test_preds = model.predict(X_test)\n",
    "            \n",
    "            model_name = model.get_model_name()\n",
    "            all_scores[model_name] = fold_scores\n",
    "            oof_predictions[model_name] = oof_preds\n",
    "            test_predictions[model_name] = test_preds\n",
    "            \n",
    "            # Cleanup\n",
    "            del model.trainer\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save base model predictions\n",
    "        oof_df = pd.DataFrame(oof_predictions)\n",
    "        test_df = pd.DataFrame(test_predictions)\n",
    "        joblib.dump(oof_df, os.path.join(self.config.models_dir, \"oof_preds.pkl\"))\n",
    "        joblib.dump(test_df, os.path.join(self.config.models_dir, \"test_preds.pkl\"))\n",
    "        \n",
    "        # Train ensemble model\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING ENSEMBLE MODEL\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        ensemble_scores, coefficients = self.ensemble.train(oof_predictions, y_train['label'])\n",
    "        ensemble_test_preds = self.ensemble.predict(test_predictions)\n",
    "        \n",
    "        all_scores[\"Ridge (ensemble)\"] = ensemble_scores\n",
    "        \n",
    "        # Print results summary\n",
    "        self._print_results_summary(all_scores)\n",
    "        \n",
    "        # Visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        self.visualizar_resultados(y_train, oof_df, all_scores, coefficients, list(oof_predictions.keys()))\n",
    "        \n",
    "        # Analysis\n",
    "        self._analyze_results(y_train, oof_df, X_train)\n",
    "        \n",
    "        # Save submission\n",
    "        submission_filename = self._save_submission(ensemble_test_preds, ensemble_scores)\n",
    "                \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Submission saved as: {submission_filename}\")\n",
    "        print(f\"Final ensemble CV score: {np.mean(ensemble_scores):.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "        return submission_filename\n",
    "    \n",
    "    def visualizar_resultados(self, y_train: pd.Series, oof_df: pd.DataFrame, \n",
    "                             all_scores: Dict[str, List[float]], coefficients: np.ndarray, \n",
    "                             model_names: List[str]):\n",
    "        \"\"\"Generate all visualizations.\"\"\"\n",
    "        # Ensemble weights\n",
    "        self.visualizer.plot_ensemble_weights(\n",
    "            coefficients, \n",
    "            model_names,\n",
    "            \"Ridge Ensemble Coefficients\"\n",
    "        )\n",
    "        \n",
    "        # Model performance comparison\n",
    "        self.visualizer.plot_model_performance(all_scores)\n",
    "        \n",
    "        # Model correlations\n",
    "        correlation_matrix = self.visualizer.plot_model_correlations(oof_df)\n",
    "        print(\"\\nBase Model Correlations:\")\n",
    "        print(correlation_matrix)\n",
    "        \n",
    "        # Prediction distributions\n",
    "        self.visualizer.plot_prediction_distributions(oof_df)\n",
    "    \n",
    "    def _analyze_results(self, y_train: pd.Series, oof_df: pd.DataFrame, X_train: pd.DataFrame):\n",
    "        \"\"\"Analyze and print detailed results.\"\"\"\n",
    "        print(\"\\n=== INDIVIDUAL MODEL PERFORMANCE ===\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name, predictions in oof_df.items():\n",
    "            corr = pearsonr(y_train['label'], predictions)[0]\n",
    "            print(f\"{model_name:20s}: {corr:.6f}\")\n",
    "        \n",
    "        print(\"\\n=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Original features: {len([col for col in X_train.columns if col.startswith('X')])}\")\n",
    "        print(f\"Interaction features: {len([col for col in X_train.columns if 'interaction' in col])}\")\n",
    "        print(f\"Market features: {len([col for col in X_train.columns if any(x in col for x in ['ratio', 'imbalance', 'pressure', 'liquidity'])])}\")\n",
    "        print(f\"Total features: {X_train.shape[1]}\")\n",
    "        \n",
    "        print(\"\\n=== MEMORY OPTIMIZATION ===\")\n",
    "        print(\"-\" * 50)\n",
    "        memory_usage = X_train.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"Training data memory usage: {memory_usage:.2f} MB\")\n",
    "        \n",
    "        print(\"\\n=== GPU UTILIZATION ===\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"GPU enabled: {self.config.use_gpu}\")\n",
    "        if self.config.use_gpu:\n",
    "            print(f\"GPU device ID: {self.config.gpu_id}\")\n",
    "            print(\"Models using GPU: LightGBM, XGBoost\")\n",
    "        else:\n",
    "            print(\"Running on CPU\")\n",
    "    \n",
    "    def _print_results_summary(self, scores: Dict[str, List[float]]):\n",
    "        \"\"\"Print detailed results summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        mean_scores = scores_df.mean().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nModel Rankings (by mean CV score):\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (model, score) in enumerate(mean_scores.items(), 1):\n",
    "            std_score = scores_df[model].std()\n",
    "            print(f\"{i}. {model:20s} - Mean: {score:.6f} (¬±{std_score:.6f})\")\n",
    "        \n",
    "        print(\"\\nDetailed Fold Scores:\")\n",
    "        print(\"-\" * 40)\n",
    "        for model in mean_scores.index:\n",
    "            fold_scores = scores[model]\n",
    "            print(f\"\\n{model}:\")\n",
    "            for fold, score in enumerate(fold_scores, 1):\n",
    "                print(f\"   Fold {fold}: {score:.6f}\")\n",
    "    \n",
    "    def _save_submission(self, predictions: np.ndarray, scores: List[float]) -> str:\n",
    "        filename = \"submission.csv\"\n",
    "        filepath = os.path.join(self.config.results_dir, filename)\n",
    "        \n",
    "        predict = pd.DataFrame(data={'id': range(1, len(predictions)+1), 'target': predictions})\n",
    "        predict.to_csv(filepath, index=False)\n",
    "                \n",
    "        return filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b07f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration initialized!\n",
      "Training data path: ../kaggle/input/X_train.parquet\n",
      "Test data path: ../kaggle/input/X_test.parquet\n",
      "GPU enabled: True\n",
      "============================================================\n",
      "CRYPTO MARKET PREDICTION PIPELINE\n",
      "============================================================\n",
      "Loading data...\n",
      "Training data shape: (525887, 20)\n",
      "Test data shape: (538150, 20)\n",
      "Training y shape: (525887, 1)\n",
      "Number of features: 20\n",
      "\n",
      "============================================================\n",
      "TRAINING BASE MODELS\n",
      "============================================================\n",
      "Training LightGBM (gbdt)...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['label'], dtype='object')\n",
      "Training LGBMRegressor\n",
      "\n",
      "--- Fold 0 - _pearsonr: 0.014422 - Time: 2.83 s\n",
      "--- Fold 1 - _pearsonr: -0.168506 - Time: 2.26 s\n",
      "--- Fold 2 - _pearsonr: -0.018747 - Time: 2.28 s\n",
      "--- Fold 3 - _pearsonr: 0.054977 - Time: 2.40 s\n",
      "--- Fold 4 - _pearsonr: 0.089239 - Time: 2.17 s\n",
      "--- Fold 5 - _pearsonr: -0.066780 - Time: 2.17 s\n",
      "--- Fold 6 - _pearsonr: 0.191760 - Time: 2.22 s\n",
      "--- Fold 7 - _pearsonr: 0.190788 - Time: 2.26 s\n",
      "--- Fold 8 - _pearsonr: -0.045603 - Time: 2.21 s\n",
      "--- Fold 9 - _pearsonr: 0.087972 - Time: 2.24 s\n",
      "--- Fold 10 - _pearsonr: 0.085999 - Time: 2.26 s\n",
      "--- Fold 11 - _pearsonr: -0.005318 - Time: 2.18 s\n",
      "--- Fold 12 - _pearsonr: -0.103382 - Time: 2.20 s\n",
      "--- Fold 13 - _pearsonr: -0.024826 - Time: 2.20 s\n",
      "--- Fold 14 - _pearsonr: -0.044307 - Time: 2.20 s\n",
      "--- Fold 15 - _pearsonr: 0.022776 - Time: 2.13 s\n",
      "--- Fold 16 - _pearsonr: 0.307989 - Time: 2.08 s\n",
      "--- Fold 17 - _pearsonr: -0.161763 - Time: 2.10 s\n",
      "--- Fold 18 - _pearsonr: 0.199390 - Time: 2.07 s\n",
      "--- Fold 19 - _pearsonr: -0.036257 - Time: 2.11 s\n",
      "--- Fold 20 - _pearsonr: -0.159633 - Time: 2.07 s\n",
      "--- Fold 21 - _pearsonr: 0.039962 - Time: 2.06 s\n",
      "--- Fold 22 - _pearsonr: -0.160658 - Time: 2.07 s\n",
      "--- Fold 23 - _pearsonr: 0.101493 - Time: 2.09 s\n",
      "--- Fold 24 - _pearsonr: 0.034346 - Time: 2.07 s\n",
      "--- Fold 25 - _pearsonr: 0.064547 - Time: 2.03 s\n",
      "--- Fold 26 - _pearsonr: -0.010359 - Time: 2.08 s\n",
      "--- Fold 27 - _pearsonr: -0.068008 - Time: 2.11 s\n",
      "--- Fold 28 - _pearsonr: -0.057407 - Time: 2.07 s\n",
      "--- Fold 29 - _pearsonr: -0.031245 - Time: 2.06 s\n",
      "--- Fold 30 - _pearsonr: -0.095713 - Time: 2.10 s\n",
      "--- Fold 31 - _pearsonr: -0.026877 - Time: 2.06 s\n",
      "--- Fold 32 - _pearsonr: -0.311080 - Time: 2.04 s\n",
      "--- Fold 33 - _pearsonr: 0.143332 - Time: 2.08 s\n",
      "--- Fold 34 - _pearsonr: -0.006586 - Time: 2.07 s\n",
      "--- Fold 35 - _pearsonr: 0.258813 - Time: 2.06 s\n",
      "--- Fold 36 - _pearsonr: 0.024509 - Time: 2.09 s\n",
      "--- Fold 37 - _pearsonr: 0.175680 - Time: 2.06 s\n",
      "--- Fold 38 - _pearsonr: 0.247442 - Time: 2.05 s\n",
      "--- Fold 39 - _pearsonr: 0.246795 - Time: 2.06 s\n",
      "--- Fold 40 - _pearsonr: 0.178812 - Time: 2.07 s\n",
      "--- Fold 41 - _pearsonr: -0.023160 - Time: 2.09 s\n",
      "--- Fold 42 - _pearsonr: -0.100671 - Time: 2.14 s\n",
      "--- Fold 43 - _pearsonr: 0.269398 - Time: 2.06 s\n",
      "--- Fold 44 - _pearsonr: -0.096613 - Time: 2.07 s\n",
      "--- Fold 45 - _pearsonr: -0.131536 - Time: 2.06 s\n",
      "--- Fold 46 - _pearsonr: 0.031327 - Time: 2.10 s\n",
      "--- Fold 47 - _pearsonr: 0.111843 - Time: 2.09 s\n",
      "--- Fold 48 - _pearsonr: -0.132485 - Time: 2.08 s\n",
      "--- Fold 49 - _pearsonr: -0.070571 - Time: 2.11 s\n",
      "\n",
      "------ Overall _pearsonr: -0.000623 - Mean _pearsonr: 0.020310 ¬± 0.133665 - Time: 110.30 s\n",
      "   Fold scores: [0.0144217891185346, -0.1685060518165849, -0.018747281636692533, 0.05497679400894322, 0.08923948202211246, -0.06678012486538228, 0.19176039168667003, 0.19078794905599436, -0.04560254718081205, 0.08797186235212984, 0.08599933968543574, -0.005318375776050897, -0.10338175135903663, -0.024826050609931604, -0.04430715938386576, 0.022776287361713922, 0.30798945395960414, -0.1617628191620069, 0.19939040301200514, -0.03625672832507869, -0.15963283320838872, 0.03996210348349647, -0.1606582579522863, 0.10149301777707284, 0.03434582418441394, 0.06454679982608308, -0.010358696278471233, -0.06800824810855444, -0.05740657492268889, -0.031245403615644537, -0.09571285608214812, -0.02687684865338881, -0.3110800597778421, 0.14333188835930505, -0.006585809655922247, 0.2588131529385194, 0.024509040848278884, 0.17567965343811026, 0.2474417769972885, 0.24679538648765192, 0.17881238515753098, -0.023160168413193162, -0.10067118383312948, 0.2693975690071917, -0.09661287571257493, -0.1315360651337974, 0.03132710823889559, 0.1118429960467277, -0.1324851178580708, -0.07057101743223236]\n",
      "   Mean CV score: 0.020310\n",
      "   Std CV score: 0.133665\n",
      "Training LightGBM (goss)...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['label'], dtype='object')\n",
      "Training LGBMRegressor\n",
      "\n",
      "--- Fold 0 - _pearsonr: -0.006965 - Time: 8.24 s\n",
      "--- Fold 1 - _pearsonr: -0.173534 - Time: 8.02 s\n",
      "--- Fold 2 - _pearsonr: -0.051834 - Time: 7.98 s\n",
      "--- Fold 3 - _pearsonr: 0.074676 - Time: 8.26 s\n",
      "--- Fold 4 - _pearsonr: 0.048392 - Time: 8.07 s\n",
      "--- Fold 5 - _pearsonr: 0.017041 - Time: 8.06 s\n",
      "--- Fold 6 - _pearsonr: 0.162905 - Time: 8.06 s\n",
      "--- Fold 7 - _pearsonr: 0.265613 - Time: 8.14 s\n",
      "--- Fold 8 - _pearsonr: 0.026055 - Time: 8.05 s\n",
      "--- Fold 9 - _pearsonr: -0.059944 - Time: 8.17 s\n",
      "--- Fold 10 - _pearsonr: 0.065412 - Time: 8.32 s\n",
      "--- Fold 11 - _pearsonr: -0.002516 - Time: 8.11 s\n",
      "--- Fold 12 - _pearsonr: -0.100029 - Time: 8.10 s\n",
      "--- Fold 13 - _pearsonr: -0.083207 - Time: 8.07 s\n",
      "--- Fold 14 - _pearsonr: 0.047097 - Time: 8.14 s\n",
      "--- Fold 15 - _pearsonr: 0.059472 - Time: 8.24 s\n",
      "--- Fold 16 - _pearsonr: 0.134192 - Time: 8.22 s\n",
      "--- Fold 17 - _pearsonr: -0.200941 - Time: 8.72 s\n",
      "--- Fold 18 - _pearsonr: 0.158601 - Time: 11.50 s\n",
      "--- Fold 19 - _pearsonr: 0.019715 - Time: 10.95 s\n",
      "--- Fold 20 - _pearsonr: -0.245426 - Time: 10.54 s\n",
      "--- Fold 21 - _pearsonr: 0.045817 - Time: 10.97 s\n",
      "--- Fold 22 - _pearsonr: -0.118231 - Time: 10.37 s\n",
      "--- Fold 23 - _pearsonr: 0.147050 - Time: 11.97 s\n",
      "--- Fold 24 - _pearsonr: -0.013552 - Time: 10.97 s\n",
      "--- Fold 25 - _pearsonr: -0.035008 - Time: 12.50 s\n",
      "--- Fold 26 - _pearsonr: 0.001776 - Time: 12.34 s\n",
      "--- Fold 27 - _pearsonr: -0.047221 - Time: 11.23 s\n",
      "--- Fold 28 - _pearsonr: -0.156624 - Time: 11.50 s\n",
      "--- Fold 29 - _pearsonr: -0.058414 - Time: 11.41 s\n",
      "--- Fold 30 - _pearsonr: -0.131818 - Time: 10.77 s\n",
      "--- Fold 31 - _pearsonr: -0.053062 - Time: 10.95 s\n",
      "--- Fold 32 - _pearsonr: -0.287247 - Time: 10.83 s\n",
      "--- Fold 33 - _pearsonr: 0.113555 - Time: 10.62 s\n",
      "--- Fold 34 - _pearsonr: -0.092556 - Time: 11.39 s\n",
      "--- Fold 35 - _pearsonr: 0.245795 - Time: 10.84 s\n",
      "--- Fold 36 - _pearsonr: 0.054907 - Time: 13.74 s\n",
      "--- Fold 37 - _pearsonr: 0.154538 - Time: 34.49 s\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    print(f\"Configuration initialized!\")\n",
    "    print(f\"Training data path: {config.train_path}\")\n",
    "    print(f\"Test data path: {config.test_path}\")\n",
    "    print(f\"GPU enabled: {config.use_gpu}\")\n",
    "    \n",
    "    # Create and run predictor\n",
    "    predictor = CryptoPredictor(config)\n",
    "    submission_file = predictor.run_pipeline()\n",
    "    \n",
    "    print(f\"\\nüéâ Prediction pipeline completed!\")\n",
    "    print(f\"üìÅ Submission file: {submission_file}\")\n",
    "    print(f\"üéØ Pipeline execution completed successfully!\")\n",
    "    print(f\"üìà Best model: Ridge Ensemble\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
